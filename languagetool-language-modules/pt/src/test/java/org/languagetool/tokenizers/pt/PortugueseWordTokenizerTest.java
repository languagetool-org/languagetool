/* LanguageTool, a natural language style checker
 * Copyright (C) 2005 Daniel Naber (http://www.danielnaber.de)
 * 
 * This library is free software; you can redistribute it and/or
 * modify it under the terms of the GNU Lesser General Public
 * License as published by the Free Software Foundation; either
 * version 2.1 of the License, or (at your option) any later version.
 *
 * This library is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
 * Lesser General Public License for more details.
 *
 * You should have received a copy of the GNU Lesser General Public
 * License along with this library; if not, write to the Free Software
 * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA  02110-1301
 * USA
 */

package org.languagetool.tokenizers.pt;

import org.junit.Test;

import java.sql.Struct;
import java.util.List;

import static org.junit.Assert.assertArrayEquals;
import static org.junit.Assert.assertEquals;

/**
 * @author Tiago F. Santos
 * @since 3.6
 */

public class PortugueseWordTokenizerTest {
  final PortugueseWordTokenizer wordTokenizer = new PortugueseWordTokenizer();

  private void testTokenise(String sentence, String ...tokens) {
    assertArrayEquals(tokens, wordTokenizer.tokenize(sentence).toArray());
  }

  @Test
  public void testTokeniseBreakChars() {
    testTokenise("Isto Ã©\u00A0um teste", new String[]{"Isto", " ", "Ã©", "Â ", "um", " ", "teste"});
    testTokenise("Isto\rquebra", new String[]{"Isto", "\r", "quebra"});
  }

  @Test
  public void testTokeniseHyphenNoWhiteSpace() {
    testTokenise("Agora isto sim Ã©-mesmo!-um teste.",
      new String[]{"Agora", " ", "isto", " ", "sim", " ", "Ã©", "-", "mesmo", "!", "-", "um", " ", "teste", "."});
  }

  @Test
  public void testTokeniseWordFinalHyphen() {
    testTokenise("Agora isto Ã©- realmente!- um teste.",
      new String[]{"Agora", " ", "isto", " ", "Ã©", "-", " ", "realmente", "!", "-", " ", "um", " ", "teste", "."});
  }

  @Test
  public void testTokeniseMDash() {
    testTokenise("Agora isto Ã©â€”realmente!â€”um teste.",
      new String[]{"Agora", " ", "isto", " ", "Ã©", "â€”", "realmente", "!", "â€”", "um", " ", "teste", "."});
  }

  @Test
  public void testTokeniseHyphenatedSingleToken() {
    testTokenise("sex-appeal", new String[]{"sex-appeal"});
    testTokenise("Aix-en-Provence", new String[]{"Aix-en-Provence"});
    testTokenise("Montemor-o-Novo", new String[]{"Montemor-o-Novo"});
    testTokenise("Andorra-a-Velha", new String[]{"Andorra-a-Velha"});
    testTokenise("TsÃ©-Tung", new String[]{"TsÃ©-Tung"});
  }

  @Test
  public void testTokeniseHyphenatedSplitRegardlessOfLetterCase() {
    testTokenise("jiu-jitsu", "jiu-jitsu");
    testTokenise("Jiu-jitsu", "Jiu-jitsu");
    testTokenise("JIU-JITSU", "JIU-JITSU");
    testTokenise("Jiu-Jitsu", "Jiu-Jitsu");
    testTokenise("franco-prussiano", "franco-prussiano");
    testTokenise("Franco-prussiano", "Franco-prussiano");
    testTokenise("Franco-Prussiano", "Franco-Prussiano");
  }

  @Test
  public void testTokeniseHyphenatedSplit() {
    testTokenise("Paris-SÃ£o Paulo", new String[]{"Paris", "-", "SÃ£o", " ", "Paulo"});
    // this word exists in the speller but not the tagger dict; this may become a problem
    testTokenise("Sem-Peixe", new String[]{"Sem", "-", "Peixe"});
    testTokenise("hÃºngaro-americano", new String[]{"hÃºngaro", "-", "americano"});
  }

  @Test
  public void testTokeniseHyphenatedClitics() {
    // As of dict v0.13!
    testTokenise("diz-se", new String[]{"diz-se"});
    testTokenise("amamo-lo", new String[]{"amamo-lo"});
    testTokenise("fi-lo", new String[]{"fi-lo"});
    testTokenise("pusÃ©-lo", new String[]{"pusÃ©-lo"});
    testTokenise("canta-lo", new String[]{"canta-lo"}); // cantas + o, may not be in speller!
    // pretty rare, but we need to generate these because the 'nos' in 'no-lo' triggers elision in -mos forms >:(
    testTokenise("dar-no-lo", new String[]{"dar-no-lo"});
    // rare contractions like these are NOT generated
    testTokenise("dÃª-mo", new String[]{"dÃª", "-", "mo"});
  }

  @Test
  public void testTokeniseMesoclisis() {
    // As of dict v0.13!
    testTokenise("fÃ¡-lo-Ã¡", new String[]{"fÃ¡-lo-Ã¡"});
    testTokenise("dir-lhe-ia", new String[]{"dir-lhe-ia"});
    testTokenise("banhar-nos-emos", new String[]{"banhar-nos-emos"});
  }

  @Test
  public void testTokeniseProductivePrefixes() {
    // These are specifically forms that are NOT in the tagger dict (though they might be in the speller).
    // The idea is that our word tagger should be able to tag them by identifying the prefix.
    testTokenise("soto-pÃ´r", new String[]{"soto-pÃ´r"});  // speller, but not tagger
    testTokenise("soto-trepar", new String[]{"soto-trepar"});  // neither speller nor tagger
  }

  @Test
  public void testTokeniseApostrophe() {
    // is
    testTokenise("d'Ã¡gua", new String[]{"d", "'", "Ã¡gua"});
    testTokenise("dâ€™Ã¡gua", new String[]{"d", "â€™", "Ã¡gua"});
    // should be
    // testTokenise("dâ€™Ã¡gua", new String[]{"dâ€™", "Ã¡gua"});
    // testTokenise("d'Ã¡gua", new String[]{"d'", "Ã¡gua"});
  }

  @Test
    public void testTokeniseHashtags() {
    // Twitter and whatnot; same as English
    testTokenise("#CantadasDoBem", new String[]{"#", "CantadasDoBem"});
  }

  @Test
  public void testDoNotTokeniseUserMentions() {
    // Twitter and whatnot; same as English
    testTokenise("@user", "@user");
  }

  @Test
  public void testTokeniseCurrency() {
    testTokenise("R$45,00", new String[]{"R$", "45,00"});
    testTokenise("5Â£", new String[]{"5", "Â£"});
    testTokenise("US$249,99", new String[]{"US$", "249,99"});
    testTokenise("â‚¬2.000,00", new String[]{"â‚¬", "2.000,00"});
  }

  @Test
  public void testTokeniseSplitsPercent() {
    testTokenise("50%", "50%");
    testTokenise("50%%", "50%", "%"); // "%" is a right-edge character that can repeat ONCE
    testTokenise("50%OFF", "50%", "OFF");
    testTokenise("%50", "%", "50");
    testTokenise("%", "%");
  }

  @Test
  public void testTokeniseNumberAbbreviation() {
    testTokenise("NÂº666", new String[]{"NÂº666"});  // ordinal indicator 'o'
    testTokenise("NÂ°666", new String[]{"NÂ°666"});  // degree symbol
    testTokenise("NÂº 420", new String[]{"NÂº", " ", "420"});
    testTokenise("N.Âº69", new String[]{"N", ".", "Âº69"});  // the '.' char splits it
    testTokenise("N.Âº 80085", new String[]{"N", ".", "Âº", " ", "80085"});  // the '.' char splits it
  }

  @Test
  public void testDoNotTokeniseOrdinalSuperscript() {
    // ordinal indicators
    testTokenise("1Âº", new String[]{"1Âº"});
    testTokenise("2.Âº", new String[]{"2.Âº"});
    testTokenise("3ÂºË¢", new String[]{"3ÂºË¢"});
    testTokenise("4.ÂºË¢", new String[]{"4.ÂºË¢"});
    testTokenise("5Âª", new String[]{"5Âª"});
    testTokenise("6Âª", new String[]{"6Âª"});
    testTokenise("7ÂªË¢", new String[]{"7ÂªË¢"});
    testTokenise("8ÂªË¢", new String[]{"8ÂªË¢"});
    // superscripts
    testTokenise("9áµ’", new String[]{"9áµ’"});
    testTokenise("10.áµ’", new String[]{"10.áµ’"});
    testTokenise("11áµ’Ë¢", new String[]{"11áµ’Ë¢"});
    testTokenise("12.áµ’Ë¢", new String[]{"12.áµ’Ë¢"});
    testTokenise("13áµƒ", new String[]{"13áµƒ"});
    testTokenise("14.áµƒ", new String[]{"14.áµƒ"});
    testTokenise("15áµƒË¢", new String[]{"15áµƒË¢"});
    testTokenise("16.áµƒË¢", new String[]{"16.áµƒË¢"});
    // regular lowercase
    testTokenise("17o", new String[]{"17o"});
    testTokenise("18.o", new String[]{"18.o"});
    testTokenise("19os", new String[]{"19os"});
    testTokenise("20.os", new String[]{"20.os"});
    testTokenise("21a", new String[]{"21a"});
    testTokenise("22.a", new String[]{"22.a"});
    testTokenise("23as", new String[]{"23as"});
    testTokenise("24.as", new String[]{"24.as"});
  }

  @Test
  public void testDoNotTokeniseDegreeExpressions() {
    testTokenise("25Â°", new String[]{"25Â°"});
    testTokenise("26,0Â°", new String[]{"26,0Â°"});
    testTokenise("27.0Â°", new String[]{"27.0Â°"});
    testTokenise("28,0Â°C", new String[]{"28,0Â°C"});
    testTokenise("29.0Â°C", new String[]{"29.0Â°C"});
    testTokenise("30,0Â°c", new String[]{"30,0Â°c"});
    testTokenise("31.0Â°c", new String[]{"31.0Â°c"});
    testTokenise("32Â°Ra", new String[]{"32Â°Ra"});
    testTokenise("33,1Â°RÃ¸", new String[]{"33,1Â°RÃ¸"});
    testTokenise("34Â°N", new String[]{"34Â°N"});
  }

  @Test
  public void testDoNotTokeniseSpaceSeparatedThousands() {
    testTokenise("35 000", new String[]{"35 000"});
    testTokenise("36 000 000", new String[]{"36 000 000"});
    testTokenise("37 000,00", new String[]{"37 000,00"});
    testTokenise("38 000 000,00", new String[]{"38 000 000,00"});
    testTokenise("39 000Â°", new String[]{"39 000Â°"});
    testTokenise("40 000%", new String[]{"40 000%"});
    testTokenise("41 000Âº", new String[]{"41 000Âº"});
    testTokenise("42 000o", new String[]{"42 000o"});
    testTokenise("43 00", new String[]{"43", " ", "00"});
  }

  @Test
  public void testTokeniseExponent() {
    testTokenise("kmÂ²", new String[]{"km", "Â²"});
  }

  @Test public void testTokeniseCopyrightAndSimilarSymbols() {
    testTokenise("CopyrightÂ©", new String[]{"Copyright", "Â©"});
    testTokenise("BacanaÂ®", new String[]{"Bacana", "Â®"});
    testTokenise("Legalâ„¢", new String[]{"Legal", "â„¢"});
  }

  @Test
  public void testTokeniseEmoji() {
    testTokenise("â˜ºâ˜ºâ˜ºSÃ³", "â˜º", "â˜º", "â˜º", "SÃ³");
  }

  @Test
  public void testDoNotTokeniseModifierDiacritics() {
    // the tilde here is a unicode modifier char; normally, the unicode a-tilde (Ã£) is used
    testTokenise("NaÌƒo", "NaÌƒo");
  }

  @Test
  public void testTokeniseExtraWordEdgeChars() {
    // left-edge
    testTokenise("@50", "@50");  // single char
    testTokenise("@@50", "@", "@50");  // two chars
    testTokenise("50@", "50", "@");  // wrong edge
    testTokenise("666@50", "666", "@50");  // middle of the word
    // right-edge
    testTokenise("50â€°", "50â€°");  // single char
    testTokenise("50â€°â€°", "50â€°", "â€°");  // two chars
    testTokenise("â€°50", "â€°", "50");  // wrong edge
    testTokenise("50â€°666", "50â€°", "666");  // middle of the word
  }

  @Test
  public void testTokeniseRarePunctuation() {
    testTokenise("âŒˆHerÃ³iâŒ‹", new String[]{"âŒˆ", "HerÃ³i", "âŒ‹"});
    testTokenise("â€³Santo AntÃ´nio do Mangaâ€³", new String[]{"â€³", "Santo", " ", "AntÃ´nio", " ", "do", " ", "Manga", "â€³"});
  }

  @Test
  public void testTokeniseParagraphSymbol() {
    testTokenise("Â§1Âº", "Â§", "1Âº");
  }

  @Test
  public void testTokeniseComplexEmoji() {
    testTokenise("ğŸ§ğŸ½â€â™€ï¸", "ğŸ§", "ğŸ½", "â€", "â™€ï¸");
  }
}
